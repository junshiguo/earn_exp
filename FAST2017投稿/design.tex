\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\section{System Framework}\label{sec:SDI}
We mainly illustrate how \textcolor{red}{EARNCache} works in this section. Firstly we present the overview about the cache-earning mechanism of \textcolor{red}{EARNCache}, and then illustrate its architecture design, and finally explains the incremental cache-earning policy and its implementation.

\subsection{Overview}\label{sec:overview}

On shared non-big clusters with relatively limited cache capacity, cache resource conflicts \textcolor{red}{\sout{on non-big clusters}} would be \textcolor{yellow}{norm rather than exception}. When only quite few users are using a non-big cluster and the competition for cache resources is mild, applying on-demand caching could expedite hotter blocks taking over cache resources from colder blocks, and hotter data is less likely to be cached out by colder data. When more concurrent users are using the cluster and the competition for cache resources gets wild, on-demand caching leaves concurrently cache resource demands unmediated, which makes hotter data more vulnerable to being cached out. Files which are frequently accessed recently sometimes could be totally cached out by files which would rarely be accessed for the second time in the near future, then these hot cached-out files need caching in soon as their next access should occur in the incoming future. We consequently need to revisit existing on-demand caching mechanisms and strategies, and propose more effective measures to improve the efficiency of data caching on non-big clusters.

We believe that a good caching strategy for non-big clusters should be self-adaptive to resource competition conditions to depress competitions and avoid thrashings when resources are in desperate deficit. Obviously caching big data files on demand as a whole could not provide such self-adaptivity. Intuitively we should not cache files entirely \textcolor{red}{\sout{when caching them entirely causes tension. }}
Not compulsorily caching files entirely provides the elasticity of tuning the amount of cache resources allocated for different files, according to the files' access recency and frequency.

Ideally, more \textcolor{red}{recently frequently accessed files} should be assigned with more cache resources, \textcolor{red}{and less recently frequently accessed files}
%and less frequently accessed files recently 
should be assigned with less cache resources. However, \textcolor{red}{it's not possible to know in advance what files would be frequently accessed in the upcoming future and we could only make predictions based on historical file access patterns, especially the most recent information.}
%it's impossible to know for sure what files would be frequently accessed files recently, and we could only make predictions based on available historical file access information, especially recent accesses. 
Based on files' historical access information, \textcolor{red}{EARNCache} implements an incremental caching strategy, where a user should earn resources to cache files from other concurrent users via accessing these files. Cache resources are incrementally allocated to files becoming frequently accessed, which gradually takes over cache resources from files getting less frequently accessed, until all blocks of the file have been cached in. The more a \textcolor{red}{file} is accessed, the more cache resources it takes over. The incremental caching strategy ensures that cached hot files being frequently accessed recently will not be flushed out by other massive less hot files, which may only be accessed occasionally or randomly.

\subsection{Architecture}\label{sec:Arch}
Cached files originally are stored in the under distributed file system (i.e. Hadoop File System), and \textcolor{red}{EARNCache} \textcolor{red}{\sout{globally}} coordinately cache them across the whole cluster. \textcolor{red}{EARNCache}'s architecture consists of a central \emph{master} and a set of \emph{workers} residing on storage nodes of the cluster(see Fig.~\ref{fig:Arch}). 
The master's main role is: ~1) to determine how many cache resources should be allocated to a file, based on the resource competitions; ~2) to inform workers of cache resource allocation plans via heartbeats; ~3) to \textcolor{red}{keep} track of cache metadata about which storage node a cached block resides on; ~4) to answer clients' queries on cache metadata. \textcolor{red}{And} the worker's main role is: ~1) to receive resource allocation plans from the master; ~2) to calculate how many resources a file should contribute to compose the allocated resources; ~3) to cache in/out blocks according to the calculated resource composition plans; ~4) to inform the master of cached blocks via heartbeats; ~5) serve clients with cached blocks.

The procedures of a client accessing a block are: ~1) the client queries the master where the block is located; ~2) the master tells the client which worker it should contact to access the block; ~3) the client contacts the worker to access the block; ~4) the worker serves the client with the block data from cache. One thing worth noting here is that: the client will not contact any worker to access a block if the block has been cache out, as the master only keeps track of cached blocks, and could not provide the client with information about \textcolor{red}{cached-out blocks}, then the client has to turn to the under file system for this block.


\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.4]{figures/architecture.eps}
\caption{EARNCache's Architecture.}
\label{fig:Arch}
\end{figure}

%Competition for computational resources rarely influences the completion of running tasks, namely each of the scheduled concurrent tasks may run as fast as it runs alone. does not have obviously decrease computation efficiency, as ; while competition for cache resources may seriously decrease cache efficiency, because of data caching in/out cost.
%Big data applications usually access files in a scan fashion and datasets are usually prohibitively huge in size.
%Work about on-demand caching mainly focuses on cache replacement policies, and most replacement policies are variants of LRU, LFU or LRU-LFU-combined.

\subsection{Incremental Caching}\label{sec:framework master}

As we prefer to letting frequently accessed files recently incrementally take over resources from less frequently accessed files recently, "recently" should be defined quantitatively before we could design the incremental caching strategy, and other related elements should also be defined. Tab.~\ref{tab:notation} presents all definitions of notations involved in our incremental caching strategy.
\begin{table}[!htb]
	\caption{Notation definitions}
	\label{tab:notation}
	\centering
	\begin{tabular}{|p{0.15\linewidth}|p{0.75\linewidth}|}
		\hline
		Notation & Definition\\
		\hline
		$W$ & predefined window size of the most recently accessed data for observing files falling within\\
		\hline
		$a,b$ & scan time per unit data from memory(a) and hdd(b) \\
		\hline
		$N$ & total number of files falling in the observation window\\
		\hline
		$d_i$ & data size of the $i$th file\\
		\hline
		$D$ & total data size of $N$ files\\
		\hline
		$M$ & cache capacity of the whole cluster\\
		\hline
		$f_i$ & access frequency of the $i$th file\\
		\hline
		$F$ & total access frequency of $N$ files\\
		\hline
		$x_i$ & percentage of data cached for the $i$th file\\
		\hline
		$h_i(x_i)$ & the $i$th file's profit gain with $x_i$ data cached\\
		\hline
	\end{tabular}
\end{table}

We define a function $h_i(x_i)$ to denote the profit gain of the $i$th file to instruct how cached files should contribute resources. Then we attempt to instantiate $h_i(x_i)$ and to maximize total profit gain of all files falling in the observation window, just as Equ.\ref{optimization target} shows.
\begin{equation}\label{optimization target}
\sum_{i=1}^{N} f_i \cdot h_i(x_i) \
\end{equation}

According to definitions in Tab.\ref{tab:notation}, we can assume that the time it takes to scan the $i$th file is:
\begin{equation}\label{time}
time(x_i)=[a\cdot x_i+b\cdot (1-x_i)]\cdot d_i
\end{equation}

As mentioned above, we use $h_i$ to indicate the $i$th file's profit gain with $x_i$ data cached. If we take saved scan time as a file's profit gain, then we can define $h_i$'s deviation at $x_i$ as its gain change over $\delta x_i$, which could be further defined as the percentage of increased saving of the file's scan time with increased cache share at $x_i$ over the total saved scan time at $x_i$, compared to zero cache share, just formulized as:
\begin{equation}\label{dhi}
\frac{\delta h_i}{\delta x_i}=\frac{time(x_i) - time(x_i+\delta x_i)}{time(0) - time(x_i)}=\frac{\delta x_i}{x_i}
\end{equation}

Thus we can derive that $h_i(x_i)=\ln x_i$, and now our optimization goal becomes
\begin{equation}
\sum_{i=1}^{N} f_i \cdot \ln x_i
\end{equation}
subjected to
\begin{equation}
\sum_{i=1}^{N} x_i\cdot d_i \leq M
\end{equation}

Note at any given time,  $x_i$ is the only variant contained in the optimization goal, and $f_i \cdot \ln x_i$ is a convex function. After applying Lagrange multiplier method, our maximizing goal turns to:
\begin{equation}
L=\sum_{i=1}^{N} f_i\cdot \ln x_i - \lambda (\sum_{i=1}^{N} x_i\cdot d_i - M)
\end{equation}
Let $\frac{\delta L}{\delta x_i}$ be 0, then we get
\begin{equation}\label{xi result}
x_i\cdot d_i= \frac{f_i}{F} \cdot M
\end{equation}

The above result shows that the amount of memory resources allocated to a file is linear to $f_i$ at a given moment, as all files' access frequency is determined at that given moment, which exactly responds to our original intention of incremental caching. One more thing worth noting is that: if the overall size of files filling in the whole observation window is less than the cache capacity, which means that there are cache resources being occupied by files falling out of the observation window, \textcolor{red}{EARNCache} will collect resources from those obsolete files falling out of the observation window by LRU when a file needs caching, and the file needs caching could cache in its blocks once and for all, rather than gradually taking over resources from files falling within the observation window. \textcolor{red}{EARNCache} thereby could adaptively devolve to traditional on-demand-caching so as to expedite the process of collecting cache resources for files that need caching when the contention for cache resources is minute, and evolve to incremental caching to depress competition when resources are in deficit.

\subsection{Implementation Details}\label{sec:share and fair}

We implemented \textcolor{red}{EARNCache} by implanting our incremental caching mechanism into the modified Tachyon\textcolor{red}{\cite{tachyon}}. In \textcolor{red}{EARNCache}, we first evenly re-distribute a file's cached data blocks across the whole cluster, so that almost the same amount of blocks are hosted in cache on each cluster node, and all workers can manage their cache resources independently yet still in concert. As uneven data distribution will drag down completion of the whole job, evenly distributing cached data blocks guarantee that tasks running on each node could ideally finish almost simultaneously.

When the $i$th file needs caching, \textcolor{red}{EARNCache} pre-allocate ${f_i}/{F}$ fraction of cache resources on each node to the file based on Equ.\ref{xi result}. If the resources pre-allocated to the file is larger than its aggregate resource demand, \textcolor{red}{EARNCache} has other files in need of cache resources fairly share the resources beyond the file's actual need. Each worker checks its remaining free cache resources, and allocates as many resources as necessary to them directly if enough free resources are available, which could make full use of cache resources. When there are not enough remaining resources, the worker calls \emph{BlocksToEvict()}, which implements the eviction algorithm with incremental caching, to determine which blocks should be cached out. As all blocks are cached in from the underlying file system, cached-out blocks need no more backup and workers could discard them directly from cache. After blocks being cached in/out, workers informs the master of cached-in/out blocks, and then the master updates the metadata about caching.

Alg.~\ref{worker algorithm} describes the process of caching out blocks. \textcolor{red}{EARNCache} first checks whether the file requesting cache resources has used up its resource share in Line 1$\sim$3. In the while loop, Line 7$\sim$14 mainly selects files whose occupied cache resources exceed the most than their computed shares. If no such file exists, \textcolor{red}{EARNCache} will reject the cache resource request (Line 15$\sim$17). Otherwise, blocks of these selected files are added to the candidate cached-out blocks until enough cache resources have been collected(Line 18$\sim$24). As the same recency and frequency of all blocks within a file are identical, there exists no difference between blocks of the same file for \textcolor{red}{EARNCache} workers when choosing cached-out blocks, and workers could cache out any in-cache block of the file.

%By snatching cache resource from most over-consumed files, EarnCache ensures that increased cache occupation of file $r$ causes less degradation of other files' cache amount considering the pre-allocation plan.

\begin{algorithm}[!htb]
	\caption{Eviction Algorithm: BlocksToEvict()}
	\label{worker algorithm}
	\begin{algorithmic}[1]
		\Require \emph{s}, requested cache resources; \emph{r}, the requesting file id; \emph{A=\{$a_1$, $a_2$...$a_N$\}}, a list of files' pre-allocated memory bytes; \emph{C=\{$c_1$, $c_2$...$c_N$\}}, a list of current consumed memory bytes in local node; \emph{M}, memory capacity of local node
		\Ensure a list of candidate blocks to evict
			
		\If {$c_r \ge a_r$}
			\State algorithm ends as file r has already consumed all its allocated memory
		\EndIf
		\State $candidate \gets \{\}$ \Comment \textit{candidate cached out blocks}
		\State $mem \gets 0$ \Comment \textit{free resources obtained from evicting candidates}
		\While {$mem < s$}
		\State $j \gets -1$
		\State $over_j \gets 0$
		\For {$a_i$ in $A$ and $i \ne r$}
			\If {$c_i - a_i > over_j$}
				\State $j \gets i$
				\State $over_j \gets c_i - a_i$
			\EndIf
		\EndFor
		\If {$j = -1$}
			\State return as request failure
		\EndIf
		\State find $b_j$ as a block of file $j$ and not in $candidate$
		\State $candidate \gets candidate + b_j$
		\State $mem \gets mem + size of (b_j)$
		\State $c_j \gets c_j - sizeof(b_j)$
		\If {$mem \ge s$}
			\State return $candidate$
		\EndIf
		\EndWhile
		
	\end{algorithmic}
\end{algorithm}

%\begin{figure}[!htb]
%	\centering
%	\label{fig:decision-making}
%	\includegraphics[scale=0.70]{figures-final/decision-making.pdf}
%	\caption{The decision-making process of EarnCache when a new cache space request occurs.}
%\end{figure}


