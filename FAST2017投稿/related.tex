\section{Related Work}\label{sec:RelatedWork}

More and more time-critical applications~\cite{redis,memcached} are leveraging massive memory resources to store or cache data to gain improved data access performance, such as J. Ousterhout proposed RAMCloud~\cite{ramcloud} to keep data entirely stored in memory for large-scale Web applications, and Spark~\cite{spark,rdd} enables in-memory MapReduce~\cite{mapreduce}-style parallel computing by leveraging memory to store and cache distributed (intermediate) datasets. While caching on distributed parallel systems is tremendously different from traditional centralized page-based file system or database caching, and directly applying centralized caching on distributed parallel systems usually does not help much to improve and sometimes even hurts cache efficiency and performance.

Some previous work focuses on implementing an additional layer on existing distributed file system, which makes applications able to transparently cache distributed datasets from the underlying distributed file system. J. Zhang, et al.~\cite{dist-cache-hdfs} and Y. Luo, et al.~\cite{rcss} respectively proposed HDCache and RCSS distributed cache system based on HDFS~\cite{hdfs1,hdfs2}, which manages cached data just as HDFS manages disk data. H. Li, et al.~\cite{tachyon} further implemented a distributed memory file system for data caching by checkpointing data to the underlying file system. EARNCache imbeds the incremental caching into Tachyon~\cite{tachyon} to coordinate resource competitions and avoid cache thrashings, which improves cache efficiency and resource fairness to a certain degree.

Some work focuses on optimizing data caching for specific frameworks or goal. S. Zhang, et al.~\cite{mapreduce-cache} proposed to cache MapReduce intermediate data to speed up MapReduce applications. G. Ananthanarayanan, et al.\cite{pacman} found the important All-or-Nothing property, which implies all or none input data blocks of tasks within the same wave should be cached, and then proposed PACMan to coordinate memory caching for parallel jobs, including LIFE and LFU-F the two block cache eviction policies, where LIFE aims to minimize average completion time of jobs by favoring input files with smaller waves, and LFU-F aims to maximize cluster efficiency by favoring files that are accessed more frequently. Y. Li, ~et al.\cite{rate-aware}, S. Tang, et al.~\cite{ltrf} and A. Ghodsi, et al.~\cite{drf} respectively proposed dynamic resource partition strategies to improve fairness, and maximize the overall performance in the meanwhile. Q. Pu, et al.~\cite{fairride} extended the MAX-MIN fairness~\cite{maxmin,maxmin2} with probabilistic blocking, and proposed FairRide to avoid cheating and improve fairness for shared cache resources.

