\section{Related Work}\label{sec:RelatedWork}

There has been extensive work on memory storage and caching, as more and more time-critical applications~\cite{redis,memcached} require to store or cache data in memory to gain improved data access performance, such as J. Ousterhout, et al proposed RAMCloud~\cite{ramcloud} to keep data entirely stored in memory for large-scale Web applications, and Spark~\cite{spark,rdd} enables in-memory MapReduce~\cite{mapreduce}-style parallel computing by leveraging memory to store and cache distributed (intermediate) datasets.

%While caching on distributed parallel systems is tremendously different from traditional centralized page-based file system or database caching, and directly applying centralized caching usually does not help much to improve and sometimes even hurts cache efficiency and performance.

Some previous work focuses on implementing an additional layer on existing distributed file system, which enables applications to cache distributed datasets from the underlying distributed file system. J. Zhang, et al.~\cite{dist-cache-hdfs} and Y. Luo, et al.~\cite{rcss} respectively proposed the HDCache and RCSS distributed cache system based on HDFS~\cite{hdfs1,hdfs2}, which manages cached data just as HDFS manages disk data. H. Li, et al.~\cite{tachyon} further implemented a distributed memory file system for data caching by checkpointing data to the underlying file system. EARNCache imbeds the incremental caching into Tachyon~\cite{tachyon} to coordinate resource competitions and avoid cache thrashings, and improves cache efficiency and resource fairness to a certain degree.

Some work focuses on optimizing data caching for specific frameworks or goals. S. Zhang, et al.~\cite{mapreduce-cache} proposed to cache MapReduce intermediate data to speed up MapReduce applications. G. Ananthanarayanan, et al.\cite{pacman} found the important All-or-Nothing property, which implies all or none input data blocks of tasks within the same wave should be cached, and then proposed PACMan to coordinate memory caching for parallel jobs. Y. Li, ~et al.\cite{rate-aware}, S. Tang, et al.~\cite{ltrf} and A. Ghodsi, et al.~\cite{drf} respectively proposed dynamic resource partition strategies to improve fairness, and maximize the overall performance in the meanwhile. Q. Pu, et al.~\cite{fairride} extended the MAX-MIN fairness~\cite{maxmin,maxmin2} with probabilistic blocking, and proposed FairRide to avoid cheating and improve fairness for shared cache resources.

